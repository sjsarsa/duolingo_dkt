{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# This has been slightly modified for duolingo competition\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import time\n",
    "import csv\n",
    "import random\n",
    "from sklearn.metrics import mean_squared_error, r2_score, f1_score, accuracy_score, log_loss\n",
    "from sklearn import metrics\n",
    "from math import sqrt\n",
    "# flags\n",
    "# tf.flags.DEFINE_float(\"epsilon\", 0.1, \"Epsilon value for Adam Optimizer.\")\n",
    "# tf.flags.DEFINE_float(\"l2_lambda\", 0.3, \"Lambda for l2 loss.\")\n",
    "# tf.flags.DEFINE_float(\"learning_rate\", 0.1, \"Learning rate\")\n",
    "# tf.flags.DEFINE_float(\"max_grad_norm\", 20.0, \"Clip gradients to this norm.\")\n",
    "# tf.flags.DEFINE_float(\"keep_prob\", 0.6, \"Keep probability for dropout\")\n",
    "# tf.flags.DEFINE_integer(\"hidden_layer_num\", 1, \"The number of hidden layers (Integer)\")\n",
    "# tf.flags.DEFINE_integer(\"hidden_size\", 200, \"The number of hidden nodes (Integer)\")\n",
    "# tf.flags.DEFINE_integer(\"evaluation_interval\", 5, \"Evaluate and print results every x epochs\")\n",
    "# tf.flags.DEFINE_integer(\"batch_size\", 32, \"Batch size for training.\")\n",
    "# tf.flags.DEFINE_integer(\"epochs\", 150, \"Number of epochs to train for.\")\n",
    "# tf.flags.DEFINE_boolean(\"allow_soft_placement\", True, \"Allow device soft device placement\")\n",
    "# tf.flags.DEFINE_boolean(\"log_device_placement\", False, \"Log placement of ops on devices\")\n",
    "# tf.flags.DEFINE_boolean(\"save_test_logits\", False, \"Save test data logits on disk\")\n",
    "# tf.flags.DEFINE_string(\"train_data_path\", 'data_fr_en/formatted_data_train.csv', \"Path to the training dataset\")\n",
    "# tf.flags.DEFINE_string(\"test_data_path\", 'data_fr_en/formatted_data_test.csv', \"Path to the testing dataset\")\n",
    "\n",
    "\n",
    "# FLAGS = tf.flags.FLAGS\n",
    "# FLAGS(sys.argv, known_only=True)\n",
    "# remaining_args = FLAGS([sys.argv[0]] + [flag for flag in sys.argv if flag.startswith(\"--\")])\n",
    "# assert(remaining_args == [sys.argv[0]])\n",
    "#FLAGS._parse_flags()\n",
    "class FLAGS:\n",
    "    kind = 'canine'\n",
    "    epsilon = 0.1\n",
    "    l2_lambda= 0.3\n",
    "    learning_rate = 0.003\n",
    "    max_grad_norm = 20.0\n",
    "    keep_prob = 0.6\n",
    "    hidden_layer_num = 1\n",
    "    hidden_size = 150\n",
    "    evaluation_interval = 5\n",
    "    batch_size = 10\n",
    "    epochs = 300\n",
    "    allow_soft_placement = True\n",
    "    log_device_placement = False\n",
    "    save_test_logits = False\n",
    "    train_data_path = 'data_es_en/formatted_data_train_w.csv'\n",
    "    test_data_path = 'data_es_en/formatted_data_test_w.csv'\n",
    "\n",
    "# for attr, value in sorted(FLAGS.__flags.items()):\n",
    "#     print(\"{}={}\".format(attr.upper(), value))\n",
    "# print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the number of rows is 6000\n",
      "The number of students is  2000\n",
      "Finish reading data\n",
      "train_max_num_problems=808, train_max_skill_num=4441\n",
      "the number of rows is 5816\n",
      "The number of students is  1938\n",
      "Finish reading data\n",
      "test_max_num_problems=1251, test_max_skill_num=5266\n",
      "Reading time 0.559 s\n",
      "Time checkpoint 0.5667 s\n",
      "Starting tf session...\n"
     ]
    }
   ],
   "source": [
    "# The code is rewritten based on source code from tensorflow tutorial for Recurrent Neural Network.\n",
    "# https://www.tensorflow.org/versions/0.6.0/tutorials/recurrent/index.html\n",
    "# You can get source code for the tutorial from\n",
    "# https://github.com/tensorflow/tensorflow/blob/master/tensorflow/models/rnn/ptb/ptb_word_lm.py\n",
    "#\n",
    "# There is dropout on each hidden layer to prevent the model from overfitting\n",
    "#\n",
    "# Here is an useful practical guide for training dropout networks\n",
    "# https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf\n",
    "# You can find the practical guide on Appendix A\n",
    "\n",
    "\n",
    "def add_gradient_noise(t, stddev=1e-3, name=None):\n",
    "    \"\"\"\n",
    "    Adds gradient noise as described in http://arxiv.org/abs/1511.06807 [2].\n",
    "\n",
    "    The input Tensor `t` should be a gradient.\n",
    "\n",
    "    The output will be `t` + gaussian noise.\n",
    "\n",
    "    0.001 was said to be a good fixed value for memory networks [2].\n",
    "    \"\"\"\n",
    "    with tf.name_scope(name, \"add_gradient_noise\", [t, stddev]) as name:\n",
    "        t = tf.convert_to_tensor(t, name=\"t\")\n",
    "        gn = tf.random_normal(tf.shape(t), stddev=stddev)\n",
    "        return tf.add(t, gn, name=name)\n",
    "\n",
    "\n",
    "class StudentModel(object):\n",
    "    def __init__(self, is_training, config):\n",
    "        self._batch_size = batch_size = FLAGS.batch_size\n",
    "        self.num_skills = num_skills = config.num_skills\n",
    "        self.hidden_size = size = FLAGS.hidden_size\n",
    "        self.num_steps = num_steps = config.num_steps\n",
    "        input_size = num_skills * 2\n",
    "\n",
    "        inputs = self._input_data = tf.placeholder(tf.int32, [batch_size, num_steps])\n",
    "        self._target_id = target_id = tf.placeholder(tf.int32, [None])\n",
    "        self._target_correctness = target_correctness = tf.placeholder(tf.float32, [None])\n",
    "        final_hidden_size = size\n",
    "        hidden_layers = []\n",
    "        for i in range(FLAGS.hidden_layer_num):\n",
    "            final_hidden_size = size / (i + 1)\n",
    "            hidden1 = tf.nn.rnn_cell.LSTMCell(final_hidden_size, state_is_tuple=True)\n",
    "            if is_training and config.keep_prob < 1:\n",
    "                hidden1 = tf.nn.rnn_cell.DropoutWrapper(hidden1, output_keep_prob=FLAGS.keep_prob)\n",
    "            hidden_layers.append(hidden1)\n",
    "\n",
    "        cell = tf.nn.rnn_cell.MultiRNNCell(hidden_layers, state_is_tuple=True)\n",
    "\n",
    "        input_data = tf.reshape(self._input_data, [-1])\n",
    "        # one-hot encoding\n",
    "        with tf.device(\"/cpu:0\"):\n",
    "            labels = tf.expand_dims(input_data, 1)\n",
    "            indices = tf.expand_dims(tf.range(0, batch_size * num_steps, 1), 1)\n",
    "            concatenated = tf.concat([indices, labels], 1)\n",
    "            inputs = tf.sparse_to_dense(concatenated, tf.stack([batch_size * num_steps, input_size]), 1.0, 0.0)\n",
    "            inputs.set_shape([batch_size * num_steps, input_size])\n",
    "\n",
    "        # [batch_size, num_steps, input_size]\n",
    "        inputs = tf.reshape(inputs, [-1, num_steps, input_size])\n",
    "        x = tf.transpose(inputs, [1, 0, 2])\n",
    "        # Reshape to (n_steps*batch_size, n_input)\n",
    "        x = tf.reshape(x, [-1, input_size])\n",
    "        # Split to get a list of 'n_steps'\n",
    "        # tensors of shape (doc_num, n_input)\n",
    "        x = tf.split(x, num_steps, 0)\n",
    "        # inputs = [tf.squeeze(input_, [1]) for input_ in tf.split(1, num_steps, inputs)]\n",
    "        # outputs, state = tf.nn.rnn(hidden1, x, dtype=tf.float32)\n",
    "        outputs, state = tf.contrib.rnn.static_rnn(cell, x, dtype=tf.float32)\n",
    "        output = tf.reshape(tensor=tf.concat(outputs, 1), shape=[-1, tf.cast(final_hidden_size, dtype=tf.int32)])\n",
    "        # calculate the logits from last hidden layer to output layer\n",
    "        sigmoid_w = tf.get_variable(\"sigmoid_w\", [final_hidden_size, num_skills])\n",
    "        sigmoid_b = tf.get_variable(\"sigmoid_b\", [num_skills])\n",
    "        logits = tf.matmul(output, sigmoid_w) + sigmoid_b\n",
    "\n",
    "        # from output nodes to pick up the right one we want\n",
    "        logits = tf.reshape(logits, [-1])\n",
    "        selected_logits = tf.gather(logits, self.target_id)\n",
    "        self._all_logits = logits\n",
    "\n",
    "        # make prediction\n",
    "        self._pred = tf.sigmoid(selected_logits)\n",
    "\n",
    "        # loss function\n",
    "        loss = tf.reduce_sum(tf.nn.sigmoid_cross_entropy_with_logits(logits=selected_logits, labels=target_correctness))\n",
    "\n",
    "        # self._cost = cost = tf.reduce_mean(loss)\n",
    "        self._cost = cost = loss\n",
    "\n",
    "    @property\n",
    "    def batch_size(self):\n",
    "        return self._batch_size\n",
    "\n",
    "    @property\n",
    "    def input_data(self):\n",
    "        return self._input_data\n",
    "\n",
    "    @property\n",
    "    def auc(self):\n",
    "        return self._auc\n",
    "\n",
    "    @property\n",
    "    def pred(self):\n",
    "        return self._pred\n",
    "\n",
    "    @property\n",
    "    def target_id(self):\n",
    "        return self._target_id\n",
    "\n",
    "    @property\n",
    "    def target_correctness(self):\n",
    "        return self._target_correctness\n",
    "\n",
    "    @property\n",
    "    def initial_state(self):\n",
    "        return self._initial_state\n",
    "\n",
    "    @property\n",
    "    def all_logits(self):\n",
    "        return self._all_logits\n",
    "\n",
    "    @property\n",
    "    def cost(self):\n",
    "        return self._cost\n",
    "\n",
    "    @property\n",
    "    def final_state(self):\n",
    "        return self._final_state\n",
    "\n",
    "\n",
    "class HyperParamsConfig(object):\n",
    "    \"\"\"Small config.\"\"\"\n",
    "    init_scale = 0.05\n",
    "    num_steps = 0\n",
    "    max_grad_norm = FLAGS.max_grad_norm\n",
    "    max_max_epoch = FLAGS.epochs\n",
    "    keep_prob = FLAGS.keep_prob\n",
    "    num_skills = 0\n",
    "\n",
    "\n",
    "def run_epoch(session, m, students, eval_op, verbose=False):\n",
    "    \"\"\"Runs the model on the given data.\"\"\"\n",
    "    epoch_start_time = time.time()\n",
    "\n",
    "    index = 0\n",
    "    pred_labels = []\n",
    "    actual_labels = []\n",
    "    all_all_logits = []\n",
    "    while index + m.batch_size < len(students):\n",
    "        batch_start_time = time.time()\n",
    "        x = np.zeros((m.batch_size, m.num_steps))\n",
    "        target_id = []\n",
    "        target_correctness = []\n",
    "        count = 0\n",
    "        for i in range(m.batch_size):\n",
    "            student = students[index + i]\n",
    "            problem_ids = student[1]\n",
    "            correctness = student[2]\n",
    "            for j in range(len(problem_ids) - 1):\n",
    "                problem_id = int(problem_ids[j])\n",
    "                label_index = 0\n",
    "                if int(correctness[j]) == 0:\n",
    "                    label_index = problem_id\n",
    "                else:\n",
    "                    label_index = problem_id + m.num_skills\n",
    "                x[i, j] = label_index\n",
    "                target_id.append(i * m.num_steps * m.num_skills + j * m.num_skills + int(problem_ids[j + 1]))\n",
    "                target_correctness.append(int(correctness[j + 1]))\n",
    "                actual_labels.append(int(correctness[j + 1]))\n",
    "\n",
    "        index += m.batch_size\n",
    "\n",
    "        pred, _, all_logits = session.run([m.pred, eval_op, m.all_logits], feed_dict={\n",
    "            m.input_data: x, m.target_id: target_id,\n",
    "            m.target_correctness: target_correctness})\n",
    "\n",
    "        for p in pred:\n",
    "            pred_labels.append(p)\n",
    "\n",
    "        all_all_logits.append(all_logits)\n",
    "\n",
    "        eta = ('Batch {} to {} ({:.3}%) completed in {:.4}s, epoch ETA: {:.4}s\\t'\n",
    "               .format(index - m.batch_size, index, index / len(students), time.time() - batch_start_time,\n",
    "                       (time.time() - batch_start_time) * ((len(students) - index) / m.batch_size)))\n",
    "        print('{}\\r'.format(eta), end='', flush=True)\n",
    "\n",
    "    rmse = sqrt(mean_squared_error(actual_labels, pred_labels))\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(actual_labels, pred_labels, pos_label=1)\n",
    "    auc = metrics.auc(fpr, tpr)\n",
    "    # calculate r^2\n",
    "    r2 = r2_score(actual_labels, pred_labels)\n",
    "    f1 = f1_score(actual_labels, np.round(pred_labels))\n",
    "    lloss = log_loss(actual_labels, pred_labels)\n",
    "    acc = accuracy_score(actual_labels, np.round(pred_labels))\n",
    "    print('\\rTotal epoch time:', time.time() - epoch_start_time)\n",
    "    return acc, lloss, f1, rmse, auc, r2, np.concatenate(all_all_logits)\n",
    "\n",
    "\n",
    "def read_data_from_csv_file(fileName, shuffle=False):\n",
    "    config = HyperParamsConfig()\n",
    "    inputs = []\n",
    "    targets = []\n",
    "    rows = []\n",
    "    max_skill_num = 0\n",
    "    max_num_problems = 0\n",
    "    with open(fileName, \"r\") as csvfile:\n",
    "        reader = csv.reader(csvfile, delimiter=',')\n",
    "        for row in reader:\n",
    "            rows.append(row)\n",
    "    index = 0\n",
    "    i = 0\n",
    "    print(\"the number of rows is \" + str(len(rows)))\n",
    "    tuple_rows = []\n",
    "    # turn list to tuple\n",
    "    while (index < len(rows) - 2):\n",
    "        problems_num = int(rows[index][0])\n",
    "        tmp_max_skill = max(map(int, rows[index + 1]))\n",
    "        if (tmp_max_skill > max_skill_num):\n",
    "            max_skill_num = tmp_max_skill\n",
    "        if (problems_num <= 2):\n",
    "            index += 3\n",
    "        else:\n",
    "            if problems_num > max_num_problems:\n",
    "                max_num_problems = problems_num\n",
    "            tup = (rows[index], rows[index + 1], rows[index + 2])\n",
    "            tuple_rows.append(tup)\n",
    "            index += 3\n",
    "    # shuffle the tuple\n",
    "\n",
    "    if shuffle:\n",
    "        random.shuffle(tuple_rows)\n",
    "    print(\"The number of students is \", len(tuple_rows))\n",
    "    print(\"Finish reading data\")\n",
    "    return tuple_rows, max_num_problems, max_skill_num + 1\n",
    "\n",
    "\n",
    "def main(unused_args):\n",
    "    start_time = time.time()\n",
    "    config = HyperParamsConfig()\n",
    "    eval_config = HyperParamsConfig()\n",
    "    timestamp = str(time.time())\n",
    "    train_data_path = FLAGS.train_data_path\n",
    "    # path to your test data set\n",
    "    test_data_path = FLAGS.test_data_path\n",
    "    # the file to store your test results\n",
    "    result_file_path = \"run_logs_{}\".format(timestamp)\n",
    "    # your model name\n",
    "    model_name = \"DKT\"\n",
    "\n",
    "    train_students, train_max_num_problems, train_max_skill_num = read_data_from_csv_file(train_data_path, shuffle=True)\n",
    "    config.num_steps = train_max_num_problems\n",
    "    print('train_max_num_problems=%d, train_max_skill_num=%d' % (train_max_num_problems, train_max_skill_num))\n",
    "\n",
    "    test_students, test_max_num_problems, test_max_skill_num = read_data_from_csv_file(test_data_path, shuffle=False)\n",
    "    config.num_skills = max(test_max_skill_num, train_max_skill_num)\n",
    "    eval_config.num_steps = test_max_num_problems\n",
    "    eval_config.num_skills = max(test_max_skill_num, train_max_skill_num)\n",
    "    print('test_max_num_problems=%d, test_max_skill_num=%d' % (test_max_num_problems, test_max_skill_num))\n",
    "    print('Reading time', round(time.time() - start_time, 3), 's')\n",
    "    with tf.Graph().as_default():\n",
    "        session_conf = tf.ConfigProto(allow_soft_placement=FLAGS.allow_soft_placement,\n",
    "                                      log_device_placement=FLAGS.log_device_placement)\n",
    "\n",
    "        global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "        # decay learning rate\n",
    "        starter_learning_rate = FLAGS.learning_rate\n",
    "        learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step, 3000, 0.96, staircase=True)\n",
    "\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate, epsilon=FLAGS.epsilon)\n",
    "        print('Time checkpoint', round(time.time() - start_time, 4), 's')\n",
    "        print('Starting tf session...')\n",
    "        with tf.Session(config=session_conf) as session:\n",
    "\n",
    "            initializer = tf.random_uniform_initializer(-config.init_scale, config.init_scale)\n",
    "            # training model\n",
    "            with tf.variable_scope(\"model\", reuse=None, initializer=initializer):\n",
    "                m = StudentModel(is_training=True, config=config)\n",
    "            # testing model\n",
    "            with tf.variable_scope(\"model\", reuse=True, initializer=initializer):\n",
    "                mtest = StudentModel(is_training=False, config=eval_config)\n",
    "\n",
    "            grads_and_vars = optimizer.compute_gradients(m.cost)\n",
    "            grads_and_vars = [(tf.clip_by_norm(g, FLAGS.max_grad_norm), v)\n",
    "                              for g, v in grads_and_vars if g is not None]\n",
    "            grads_and_vars = [(add_gradient_noise(g), v) for g, v in grads_and_vars]\n",
    "            train_op = optimizer.apply_gradients(grads_and_vars, name=\"train_op\", global_step=global_step)\n",
    "            print('Time checkpoint', round((time.time() - start_time) / 60, 4), 'min')\n",
    "            print('Running session...')\n",
    "            session.run(tf.global_variables_initializer())\n",
    "            # log hyperparameters to results file\n",
    "            with open(result_file_path, \"a+\") as f:\n",
    "                print(\"Writing hyperparameters into file\")\n",
    "                f.write(\"Hidden layer size: %d \\n\" % (FLAGS.hidden_size))\n",
    "                f.write(\"Dropout rate: %.3f \\n\" % (FLAGS.keep_prob))\n",
    "                f.write(\"Batch size: %d \\n\" % (FLAGS.batch_size))\n",
    "                f.write(\"Max grad norm: %d \\n\" % (FLAGS.max_grad_norm))\n",
    "            saver = tf.train.Saver(tf.global_variables())\n",
    "            print('Starting epochs...')\n",
    "            print('Time checkpoint', round((time.time() - start_time) / 60, 4), 'min')\n",
    "            for i in range(config.max_max_epoch):\n",
    "                print('Running epoch {}...\\r'.format(i + 1))\n",
    "                acc, lloss, f1, rmse, auc, r2, _ = run_epoch(session, m, train_students, train_op, verbose=False)\n",
    "                print(\"Epoch: %d Train Metrics:\\n acc: %.3f log loss: %.3f \\t f1: %.3f \\t rmse: %.3f \\t auc: %.3f \\t r2: %.3f \\n\" \n",
    "                      % (i + 1, acc, lloss, f1, rmse, auc, r2))\n",
    "\n",
    "                if (i + 1) % FLAGS.evaluation_interval == 0:\n",
    "                    print(\"Save variables to disk\")\n",
    "                    save_path = saver.save(session, './' + model_name)\n",
    "                    print(\"*\" * 10)\n",
    "                    print(\"Start to test model....\")\n",
    "                    acc, lloss, f1, rmse, auc, r2, all_logits = run_epoch(session, mtest, test_students, tf.no_op(), verbose=True)\n",
    "                    print(\"Epoch: %d Test Metrics:\\n acc: %.3f \\t log loss: %.3f \\t f1: %.3f \\t rmse: %.3f \\t auc: %.3f \\t r2: %.3f\" \n",
    "                          % (i + 1, acc, lloss, f1, rmse, auc, r2))\n",
    "                    with open(result_file_path, \"a+\") as f:\n",
    "                        f.write(\"Epoch: %d Test Metrics:\\n rmse: %.3f \\t auc: %.3f \\t r2: %.3f\" % (\n",
    "                            (i + 1) / 2, rmse, auc, r2))\n",
    "                        f.write(\"\\n\")\n",
    "\n",
    "                        print(\"*\" * 10)\n",
    "                    # save the logits\n",
    "                    if FLAGS.save_test_logits:\n",
    "                        np.savetxt(result_file_path + ('.e%02d.logits' % (i + 1)), all_logits)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    tf.app.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
